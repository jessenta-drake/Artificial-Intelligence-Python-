{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYYuawfChrBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f35d2d-83cb-495f-c05d-305eccf4f871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/953.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/953.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m921.6/953.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fortnite.zip\n",
        "%ls\n"
      ],
      "metadata": {
        "id": "_EJF58gshu9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78d0eb6-e6d3-47e8-b12f-f73c40a2a770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/fortnite.zip, /content/fortnite.zip.zip or /content/fortnite.zip.ZIP.\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import cv2\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Defining the Wumpus World Environment.\n",
        "class env(gym.Env):\n",
        "    def __init__(self, environment_type):\n",
        "        self.envtype = environment_type\n",
        "        self.environment_width = 6\n",
        "        self.environment_height = 6\n",
        "        self.obspace = spaces.Discrete(self.environment_width * self.environment_height)\n",
        "        self.actionspace = spaces.Discrete(4)\n",
        "        self.agent_pos = np.asarray([0, 0])\n",
        "        self.breeze_pos = np.asarray([[0,2],[5,0]])\n",
        "        self.gold_pos = np.asarray([5, 5])\n",
        "        self.gold_quantity = 1\n",
        "        self.goal_pos= np.asarray([5,5])\n",
        "        self.goal_quantity=1\n",
        "        self.max_timesteps=15\n",
        "        self.timestep=10\n",
        "\n",
        "        self.pit_pos = np.asarray([[3, 3],[2,1],[4,0]])\n",
        "\n",
        "\n",
        "        self.pit_pos = np.asarray([[4, 1], [2, 1], [0, 4], [3, 5], [1, 5]])\n",
        "        self.stench_pos = np.asarray([[3, 2], [2, 3], [4, 3], [3, 4]])\n",
        "        self.wumpus_pos = np.asarray([3, 3])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"This method resets the agent position and returns the state as the observation.\n",
        "\n",
        "        :returns int observation: -  Integer representing the grid block the agent is in.\n",
        "                 dict info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
        "        self.state = np.zeros((6,6))\n",
        "        self.state[tuple(self.agent_pos)] = 1\n",
        "        self.state[tuple(self.goal_pos)] = 0.5\n",
        "        observation = self.state.flatten()\n",
        "\n",
        "        info = {}\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"This method implements what happens when the agent takes a particular action. It changes the agent's\n",
        "        position (While not allowing it to go out of the environment space.), maps the environment co-ordinates to a\n",
        "        state, defines the rewards for the various states, and determines when the episode ends.\n",
        "\n",
        "        :param int action: - Integer in the range 0 to 3 inclusive representing the different actions the agent can\n",
        "        take.\n",
        "\n",
        "        :returns arr observation: - Array representing the partial observation.\n",
        "                 int reward: - Integer value that's used to measure the performance of the agent.\n",
        "                 bool terminated: - Boolean describing whether the episode has ended.\n",
        "                 bool truncated: Boolean describing whether a truncation condition outside the scope of the MDP is\n",
        "                                 satisfied.\n",
        "                 dict info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
        "\n",
        "        # Describing the outcomes of the various possible actions.\n",
        "        if action == 0:\n",
        "            self.agent_pos[0] += 1  # This action causes the agent to go right.\n",
        "        if action == 1:\n",
        "            self.agent_pos[0] -= 1  # This action causes the agent to go left.\n",
        "        if action == 2:\n",
        "            self.agent_pos[1] += 1  # This action causes the agent to go up.\n",
        "        if action == 3:\n",
        "            self.agent_pos[1] -= 1  # This action causes the agent to go down.\n",
        "\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, 2)\n",
        "\n",
        "\n",
        "        reward = 0\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "          reward = 1\n",
        "\n",
        "        self.timestep += 1\n",
        "\n",
        "        # Condition to check for termination (episode is over)\n",
        "        terminated = True if self.timestep >= self.max_timesteps else False\n",
        "\n",
        "        # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
        "        # This helps the agent to learn how to behave in a safe and predictable manner\n",
        "        truncated = True if np.all((self.agent_pos >=0 ) & (self.agent_pos <= 2)) else False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        reward = 0\n",
        "        rewardList=[[0,2],[0,4],[1,5],[2,1],[3,5],[4,1],[5,0],[2,3],[3,2],[3,3],[3,4],[4,3]]\n",
        "\n",
        "        if self.state == [0,2]:\n",
        "          reward+=1\n",
        "        if self.state == [0,4]:\n",
        "          reward+=2\n",
        "        if self.state == [1,5]:\n",
        "          reward+=2\n",
        "        if self.state == [2,1]:\n",
        "          reward+=2\n",
        "        if self.state == [3,5]:\n",
        "          reward+=2\n",
        "        if self.state == [4,1]:\n",
        "          reward+=2\n",
        "        if self.state == [5,0]:\n",
        "          reward+=1\n",
        "        if self.state == [2,3]:\n",
        "          reward-=1\n",
        "        if self.state == [3,2]:\n",
        "          reward-=1\n",
        "        if self.state == [3,3]:\n",
        "          reward-=2\n",
        "        if self.state == [3,4]:\n",
        "          reward-=1\n",
        "        if self.state == [4,3]:\n",
        "          reward-=1\n",
        "        return\n",
        "\n",
        "\n",
        "    def render(self, mode='human', plot=False):\n",
        "        \"\"\"This method renders the environment.\n",
        "\n",
        "        :param str mode: 'human' renders to the current display or terminal and returns nothing.\n",
        "        :param bool plot: Boolean indicating whether we show a plot or not. If False, the method returns a resized NumPy\n",
        "                     array representation of the environment to be used as the state. If True it plots the environment.\n",
        "\n",
        "        :returns arr preprocessed_image: Grayscale NumPy array representation of the environment.\"\"\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(15, 15))\n",
        "        ax.set_xlim(0, 6)\n",
        "        ax.set_ylim(0, 6)\n",
        "\n",
        "        def plot_image(plot_pos):\n",
        "            \"\"\"This is a helper function to render the environment. It checks which objects are in a particular\n",
        "            position on the grid and renders the appropriate image.\n",
        "\n",
        "            :param arr plot_pos: Co-ordinates of the grid position which needs to be rendered.\"\"\"\n",
        "\n",
        "            # Initially setting every object to not be plotted.\n",
        "            plot_agent, plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus = \\\n",
        "                False, False, False, False, False, False\n",
        "\n",
        "            # Checking which objects need to be plotted by comparing their positions.\n",
        "            if np.array_equal(self.agent_pos, plot_pos):\n",
        "                plot_agent = True\n",
        "            if any(np.array_equal(self.breeze_pos[i], plot_pos) for i in range(len(self.breeze_pos))):\n",
        "                plot_breeze = True\n",
        "            if self.gold_quantity > 0:  # Gold isn't plotted if it has already been picked by one of the agents.\n",
        "                if np.array_equal(plot_pos, self.gold_pos):\n",
        "                    plot_gold = True\n",
        "            if any(np.array_equal(self.pit_pos[i], plot_pos) for i in range(len(self.pit_pos))):\n",
        "                plot_pit = True\n",
        "            if any(np.array_equal(self.stench_pos[i], plot_pos) for i in range(len(self.stench_pos))):\n",
        "                plot_stench = True\n",
        "            if np.array_equal(plot_pos, self.wumpus_pos):\n",
        "                plot_wumpus = True\n",
        "\n",
        "            #Plot for Agent.\n",
        "            if plot_agent and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n",
        "                agent = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/agent.jpeg'), zoom=0.28),\n",
        "                                       np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(agent)\n",
        "\n",
        "            # Plot for Breeze.\n",
        "            elif plot_breeze and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n",
        "                breeze = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/medkit.jpeg'), zoom=0.28),\n",
        "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(breeze)\n",
        "\n",
        "            # Plot for Gold.\n",
        "            elif plot_gold and \\\n",
        "                    all(not item for item in [plot_pit]):\n",
        "                gold = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/chest.jpeg'), zoom=0.28),\n",
        "                                      np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(gold)\n",
        "\n",
        "\n",
        "            # Plot for Pit.\n",
        "            elif plot_pit and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_breeze]):\n",
        "                pit = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/llama.jpeg'), zoom=0.28),\n",
        "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(pit)\n",
        "\n",
        "            # # Plot for Stench.\n",
        "            elif plot_stench and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_breeze, plot_gold, plot_pit, plot_wumpus]):\n",
        "                stench = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/stickbomb.jpeg'), zoom=0.28),\n",
        "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(stench)\n",
        "\n",
        "            # Plot for Wumpus.\n",
        "            elif plot_wumpus and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_breeze, plot_gold, plot_pit, plot_stench]):\n",
        "                wumpus = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/anotherbomb.jpeg'), zoom=0.28),\n",
        "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(wumpus)\n",
        "\n",
        "        coordinates_state_mapping_2 = {}\n",
        "        for j in range(self.environment_height * self.environment_width):\n",
        "            coordinates_state_mapping_2[j] = np.asarray(\n",
        "                [j % self.environment_width, int(np.floor(j / self.environment_width))])\n",
        "\n",
        "        # Rendering the images for all states.\n",
        "        for position in coordinates_state_mapping_2:\n",
        "            plot_image(coordinates_state_mapping_2[position])\n",
        "\n",
        "        plt.xticks([0, 1, 2, 3, 4, 5])\n",
        "        plt.yticks([0, 1, 2, 3, 4, 5])\n",
        "        plt.grid()\n",
        "\n",
        "        if plot:  # Displaying the plot.\n",
        "            plt.show()\n",
        "        else:  # Returning the preprocessed image representation of the environment.\n",
        "            fig.canvas.draw()\n",
        "            img = np.array(fig.canvas.renderer.buffer_rgba())[:, :, :3]\n",
        "            width = 84\n",
        "            height = 84\n",
        "            dim = (width, height)\n",
        "            # noinspection PyUnresolvedReferences\n",
        "            preprocessed_image = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
        "            plt.show()\n",
        "            return preprocessed_image\n",
        "\n",
        "\n",
        "wumpus_world = env(environment_type='deterministic')\n",
        "wumpus_world.reset()\n",
        "wumpus_world.render(plot=True)\n",
        "wumpus_world.step(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5MIUSANTWW6r",
        "outputId": "07f57007-f4bb-4360-c6b1-3e27305ae5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/fortnite/agent.jpeg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-423711daba83>\u001b[0m in \u001b[0;36m<cell line: 238>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0mwumpus_world\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deterministic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0mwumpus_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m \u001b[0mwumpus_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0mwumpus_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-423711daba83>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, plot)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# Rendering the images for all states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoordinates_state_mapping_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates_state_mapping_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-423711daba83>\u001b[0m in \u001b[0;36mplot_image\u001b[0;34m(plot_pos)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     all(not item for item in\n\u001b[1;32m    164\u001b[0m                         [plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n\u001b[0;32m--> 165\u001b[0;31m                 agent = AnnotationBbox(OffsetImage(plt.imread('/content/fortnite/agent.jpeg'), zoom=0.28),\n\u001b[0m\u001b[1;32m    166\u001b[0m                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n\u001b[1;32m    167\u001b[0m                 \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_artist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             )\n\u001b[0;32m-> 1563\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1565\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/fortnite/agent.jpeg'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent:\n",
        "\n",
        "  def __init__(self, env):\n",
        "      self.env = env\n",
        "      self.observation_space = env.observation_space\n",
        "      self.action_space = env.action_space\n",
        "\n",
        "\n",
        "  def step(self, obs):\n",
        "    epsilon=1\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = self.env.action_space.sample()  # choose random action\n",
        "    else:\n",
        "      # choose greedy\n",
        "      action = np.argmax(Q[obs, :])  # choose action with highest Qvalue; returns index(the action)\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "As99ZzpRd1mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_values = [(0.99 ** i) * 1 for i in range(500)]\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(epsilon_values, linewidth=4)\n",
        "plt.xlabel('Episode', fontsize=28)\n",
        "plt.ylabel('Epsilon Values', fontsize=28)\n",
        "plt.title('Epsilon Decay', fontsize=36)\n",
        "plt.xticks(fontsize=24)\n",
        "plt.yticks(fontsize=24)\n",
        "plt.ylim(ymin=0, ymax=1)\n",
        "plt.xlim(xmin=0, xmax=500)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pQGJHx9zIoaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the different parameters\n",
        "epsilon = 1 #start with 1 and slowly decrease to 0.01; min val=0.01\n",
        "total_episodes = 15 #TUNED PARAMETER\n",
        "max_steps = 100 #TUNED PARAMETER\n",
        "alpha = 0.15 #btwn 0.1 - 0.2\n",
        "gamma = 0.91#btwn 0.9 - 0.99 ;\n",
        "Q = np.zeros((36,4),dtype=float)\n",
        "\n",
        "\n",
        "#helper function to choose policy\n",
        "def choose_action(state,epsilon):\n",
        "    action=0 #initialize action var\n",
        "    epsilon_list = []\n",
        "\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample() #choose random action\n",
        "    else:\n",
        "      #choose greedy\n",
        "      action = np.argmax(Q[state, :]) #choose action with highest Qvalue; returns index(the action)\n",
        "      epsilon_list.append(epsilon)\n",
        "    return action\n",
        "\n",
        "def update(state, next_state, reward, action, next_action):\n",
        "    #predicted qvalue for current state-action pair\n",
        "    curr_qval = Q[state, action]\n",
        "\n",
        "    #target q value from eqn / this would change for qlearning\n",
        "    target_qval = reward + gamma * Q[next_state, next_action]\n",
        "\n",
        "    #full eq\n",
        "    Q[state, action] = Q[state, action] + alpha * (target_qval - curr_qval)\n",
        "\n",
        "\n",
        "total_rewards_per_ep = 0\n",
        "\n",
        "def sarsa(grid, action):\n",
        "  #initializing the reward\n",
        "  reward = 0\n",
        "  total_rewards = 0\n",
        "\n",
        "  for episode in range(total_episodes):\n",
        "\n",
        "    t = 0 #count var\n",
        "    state = env.reset() #0s\n",
        "\n",
        "    #take action a\n",
        "    action = choose_action(state)\n",
        "\n",
        "    while t < max_steps:\n",
        "\n",
        "      #visualize the training\n",
        "      env.render() #supposed to render the state we're on with the sarsa algorithm\n",
        "\n",
        "      #make sure agent isn't going in a cycle\n",
        "\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      print('Action:', action, ', Reward:', reward, ', Done:', done)\n",
        "\n",
        "      if done:\n",
        "        print(state)\n",
        "        return choose_action.epsilon_list\n",
        "        return total_rewards_per_ep\n",
        "        break\n",
        "\n",
        "      next_action = choose_action(next_state,epsilon)\n",
        "\n",
        "\n",
        "      #call the RL\n",
        "      step = update(state, next_state, reward, action, next_action)\n",
        "      print(step) #show state\n",
        "\n",
        "      state = next_state\n",
        "      action = next_action\n",
        "\n",
        "      #updating our vars\n",
        "      t+=1\n",
        "\n",
        "      for x in self.reward_list:\n",
        "        if env.agent_pos == x:\n",
        "          total_rewards_per_ep+=self.reward\n",
        "           #not sure if i should update this - check if this needs to be updated. Reward does not have to be updated until position is reached\n",
        "\n",
        "\n",
        "      #if at the end of learning process\n",
        "\n",
        "\n",
        "  # while not terminated:\n",
        "  #   action = agent.step(obs)\n",
        "  #   obs, reward, terminated, truncated, info = env.step(action)\n",
        "  #   print('Action:', action, ', Reward:', reward, ', Done:', terminated)\n",
        "  #   env.render()"
      ],
      "metadata": {
        "id": "r4K1Q58TWfL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Defining the different parameters\n",
        "epsilon = 1.0  # Start with 1 and slowly decrease to 0.01; min val=0.01\n",
        "total_episodes = 15  # TUNED PARAMETER\n",
        "max_steps = 100  # TUNED PARAMETER\n",
        "alpha = 0.15  # btwn 0.1 - 0.2\n",
        "gamma = 0.91  # btwn 0.9 - 0.99\n",
        "Q = np.zeros((36, 4), dtype=float)  # Q-value table initialization\n",
        "\n",
        "env = gym.make('Taxi-v3')  # Create the Taxi-v3 environment\n",
        "\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        return env.action_space.sample()  # choose random action\n",
        "    else:\n",
        "        return np.argmax(Q[state, :])  # choose action with the highest Q-value\n",
        "\n",
        "def update(state, next_state, reward, action, next_action):\n",
        "    curr_qval = Q[state, action]\n",
        "    target_qval = reward + gamma * Q[next_state, next_action]\n",
        "    Q[state, action] = Q[state, action] + alpha * (target_qval - curr_qval)\n",
        "\n",
        "def sarsa():\n",
        "    for episode in range(total_episodes):\n",
        "        state = env.reset()\n",
        "        total_rewards_per_ep = 0\n",
        "        t = 0  # count var\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        while t < max_steps:\n",
        "            # ... (previous code remains unchanged) ...\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        epsilon *= 0.99  # Decay epsilon value after each episode\n",
        "        print(\"Episode:\", episode + 1, \"Total Reward:\", total_rewards_per_ep)\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# Run the SARSA algorithm\n",
        "sarsa()\n",
        "\n",
        "# Print the final Q-table\n",
        "print(\"Final Q-table:\")\n",
        "print(Q)\n",
        "\n"
      ],
      "metadata": {
        "id": "Fu0-Y4Ch--_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize Qa,Qb,s\n",
        "Q1 = np.zeros((36, 4), dtype=float)\n",
        "Q2 = np.zeros((36, 4), dtype=float)\n",
        "\n"
      ],
      "metadata": {
        "id": "hP4P_eXUFRm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  env = env()\n",
        "  agent = RandomAgent(env)\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  terminated, truncated = False, False\n",
        "\n",
        "  while not terminated:\n",
        "    action = agent.step(obs)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    print('Action:', action, ', Reward:', reward, ', Done:', terminated)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "1XH6XBQPcyEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}